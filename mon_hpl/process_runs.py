"""
A script that takes a directory tree generated by mon_hpl.py
and processes the data therein.
"""

import argparse, re, json, os, glob, shutil
from pathlib import Path
import numpy as np

from mon_hpl import THERMAL_CSV, CPU_CSV, RAPL_CSV, METADATA, HPLDAT, PERF_JSON
WATTS_CSV = "watts_data.csv"

def load_csv(path: Path) -> tuple[str, np.array]:
    """
    Loads a csv file and returns a tuple of format (header string, 2d array)
    """
    header = ''
    with open(path, 'r') as f:
        # grab the header
        header = next(f).rstrip()

        # check that there is actual data after the header
        try:
            next(f)
        except StopIteration as e:
            return None, None
    
    arr = np.genfromtxt(path, dtype='float', skip_header=1, delimiter=',')

    return header, arr


def write_csv(path: Path, data: np.array, header: str):
    """
    Writes the header and data array to the csv file at path
    """
    np.savetxt(path, data, header=header, fmt='%0.4f', delimiter=',', newline='\n', comments='')


def extract_timestamps(raw_data: np.array, dont_process: bool=False) -> tuple[np.array, np.array]:
    """
    Processes a 2d array and interprets the first column as unix time
    Returns two arrays, the first being a column of sample times in seconds
        and the second being the corresponding sample data
    If dont_process is True, does not convert unix timestamps to time since polling start
    """
    timestamps = raw_data[:, 0]             # get time column
    if not dont_process:
        timestamps -= timestamps[0]         # convert to time since polling start

    raw_data = raw_data[:, 1:]              # cut out time column
    return timestamps, raw_data


def rapl_to_watts(rapl_data: np.array, tsamp: float) -> np.array:
    """
    Processes a 2d array containing rapl data (energy used in uJ) and 
    returns the header along with a numpy array of power used in watts
    """

    # it is possible for the counter to overflow during a run.
    # this can cause the watts to look broken. to fix, if we detect the 
    # energy going down (impossible) then add the last known good value
    # to obscure the overflow. we assume it will never overflow twice/run
    idx = np.where(np.roll(rapl_data, 1, axis=0)[1:, :] > rapl_data[1:, :])
    if np.array(idx).size > 0:
        print("\tRAPL energy_uj counter overflow detected in input data. Correcting.")
        rows, cols = idx
        for i in range(len(cols)):
            # add pre-overflow amounts one column at a time
            rapl_data[rows[i]+1:, cols[i]] += rapl_data[rows[i], cols[i]]

    # convert micro-joules to joules
    rapl_data = rapl_data * 10 ** -6

    # convert joules to watts
    watts_data = np.gradient(rapl_data, tsamp, axis=0)
    return watts_data


def average_heterow_tables(tables: list[np.array]) -> tuple[np.array, np.array]:
    """
    Takes a list of 2d numpy arrays with non-homogeneous row counts and produces
    an averaged result along with the number of runs active per row.
    The size of the output array is equal to the size of the largest input array
    """
    # get the largest array shape. (n_rows, n_columns)
    largest_shape = (0, 0)
    for t in tables:
        if t.shape[0] > largest_shape[0]:
            largest_shape = t.shape

    # get the number of runs that have data recorded at each row
    n_running = np.zeros(largest_shape[0])
    for t in tables:
        x = np.zeros(largest_shape[0])
        x[:t.shape[0]] = 1
        n_running += x
    
    # now accumulate and average
    avg = np.zeros(largest_shape)
    for t in tables:
        x = np.zeros(largest_shape)
        x[:t.shape[0], :] = t
        avg += x

    avg = avg / n_running.reshape((len(n_running), 1))

    return avg, n_running


def average_sample_times(sample_times: list[np.array], n_running: np.array) -> np.array:
    """
    Takes a list of np arrays containing time since the start of their run
        in seconds, averages those samples into one array, and returns that array
    Uses n_running as returned by average_heterow_tables()
    """ 
    avg = np.zeros_like(n_running)
    for st in sample_times:
        x = np.zeros_like(n_running)
        x[:st.size] = st
        avg += x

    avg = avg / n_running
    return avg


def load_perf_json(path: Path):
    """
    loads a json file and returns the events as a python object
    """
    events = {}
    with open(path, 'r') as f:
        next(f)
        next(f)
        for line in f.readlines():
            data = json.loads(line)
            val = 0
            if data['counter-value'] != '<not counted>':
                val = float(data['counter-value'])
            events[data['event']] = {
                'counter-value': val,
                'event-runtime': int(data['event-runtime']),
                'pcnt-running': float(data['pcnt-running']),
            }
    return events


def average_perf_json(runs: list[dict]) -> dict:
    """
    Takes a list of perf event dictionaries and averages them into one
    """
    averaged = runs[0]
    for i in range(1, len(runs)):
        for k in runs[i].keys():
            averaged[k]['counter-value'] += runs[i][k]['counter-value']
            averaged[k]['event-runtime'] += runs[i][k]['event-runtime']
            averaged[k]['pcnt-running'] += runs[i][k]['pcnt-running']

    for k in averaged.keys():
        averaged[k]['counter-value'] /= len(runs)
        averaged[k]['event-runtime'] /= len(runs)
        averaged[k]['pcnt-running'] /= len(runs)

    return averaged


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="A script for processing the results of mon_hpl.py")
    parser.add_argument('raw_dir', type=Path, help='the directory generated by mon_hpl.py to process')
    parser.add_argument('out_dir', type=Path, help='the directory in which to write processed output')
    parser.add_argument('--leave_time_alone', action='store_true', help='When processing an output directory with n_runs=1, use this flag to prevent timestamps from being converted from unix time to time since script start')
    args = parser.parse_args()

    meta_metadata = json.loads(Path(args.raw_dir.joinpath(METADATA)).read_text())

    # check if flags make sense
    if args.leave_time_alone == True:
        if meta_metadata['n_runs'] != 1:
            raise argparse.ArgumentError("When processing a set of runs larger than N=1, cannot avoid converting raw timestamps to time since polling start")


    # set up the output directory
    if args.out_dir.exists():
        shutil.rmtree(args.out_dir)
    os.mkdir(args.out_dir)

    # copy over HPL.dat
    args.out_dir.joinpath(HPLDAT).write_text(args.raw_dir.joinpath(HPLDAT).read_text())

    # get each of the individual runs from the mon_hpl.py output directory
    runs = [Path(r) for r in glob.glob(args.raw_dir.joinpath('run_*_raw').as_posix())]

    # load in all the data
    watts_arrays = []
    watts_sampletimes = []
    therm_arrays = []
    therm_sampletimes = []
    freqs_arrays = []
    freqs_sampletimes = []
    perf_events = []

    avg_gflops = 0
    avg_runtime = 0
    for run in runs:
        # convert rapl data to watts
        rapl_header, rapl_data = load_csv(run.joinpath(RAPL_CSV))
        if rapl_data is not None:
            sampletimes, rapl_data = extract_timestamps(rapl_data, dont_process=args.leave_time_alone)
            watts_sampletimes.append(sampletimes)
            watts_arrays.append(rapl_to_watts(rapl_data, meta_metadata['t_samp']))

        # collect the other measurement csvs
        therm_header, therm_data = load_csv(run.joinpath(THERMAL_CSV))
        if therm_data is not None:
            sampletimes, therm_data = extract_timestamps(therm_data, dont_process=args.leave_time_alone)
            therm_sampletimes.append(sampletimes)
            therm_arrays.append(therm_data * 10 ** -3)    # (convert from milliC to C)
        
        freqs_header, freqs_data = load_csv(run.joinpath(CPU_CSV))
        if freqs_data is not None:
            sampletimes, freqs_data = extract_timestamps(freqs_data, dont_process=args.leave_time_alone)
            freqs_sampletimes.append(sampletimes)
            freqs_arrays.append(freqs_data)  

        # collect perf json
        if run.joinpath(PERF_JSON).exists:
            perf_events.append(load_perf_json(run.joinpath(PERF_JSON)))

        # collect metadata measurements
        metadata = json.loads(Path(run.joinpath(METADATA)).read_text())
        avg_gflops += metadata['gflops']
        avg_runtime += metadata['runtime']

    meta_metadata['avg_gflops'] = avg_gflops / len(runs)
    meta_metadata['avg_runtime'] = avg_runtime / len(runs)
    meta_metadata['processed'] = True
    meta_metadata['time_column'] = "raw timestamp" if args.leave_time_alone else "seconds since polling start"

    # now average and write out all of the runs (works for N=1 too)
    if len(watts_arrays) > 0:
        avg_watts, n_running = average_heterow_tables(watts_arrays)
        sampletimes = average_sample_times(watts_sampletimes, n_running)
        recombined = np.hstack((sampletimes[:, None], avg_watts))

        meta_metadata['power_data'] = {
            'units': 'W',
            'n_rows': len(n_running),
            'n_run_samples': n_running.tolist()
        }
        write_csv(args.out_dir.joinpath(WATTS_CSV), recombined, rapl_header)

    if len(therm_arrays) > 0:
        avg_therm, n_running = average_heterow_tables(therm_arrays)
        sampletimes = average_sample_times(therm_sampletimes, n_running)
        recombined = np.hstack((sampletimes[:, None], avg_therm))

        meta_metadata['thermal_data'] = {
            'units': 'C',
            'n_rows': len(n_running),
            'n_run_samples': n_running.tolist()
        }
        write_csv(args.out_dir.joinpath(THERMAL_CSV), recombined, therm_header)

    if len(freqs_arrays) > 0:
        avg_freqs, n_running = average_heterow_tables(freqs_arrays)
        sampletimes = average_sample_times(freqs_sampletimes, n_running)
        recombined = np.hstack((sampletimes[:, None], avg_freqs))

        meta_metadata['cpu_data'] = {
            'units': 'MHz',
            'n_rows': len(n_running),
            'n_run_samples': n_running.tolist()
        }
        write_csv(args.out_dir.joinpath(CPU_CSV), recombined, freqs_header)

    if len(perf_events) > 0:
        avg_events = average_perf_json(perf_events)
        Path(args.out_dir.joinpath(PERF_JSON)).write_text(json.dumps(avg_events, indent=4))

    # finally, update the meta-metadata
    Path(args.out_dir.joinpath(METADATA)).write_text(json.dumps(meta_metadata, indent=4))
